CFE_episodes: 1
RTH: 200000
SNR: 10
algorithm_path: agents.dqn.DQNAgent
base_config:
- ./env.yaml
- configs/base_configs/dqn.yaml
batch_size: 128
buffer_capacity: 50000
burnin_episode: 1000
env_battery_capacity: 100
env_collide_range: 1
env_comm_range: 10
env_enable_charger: false
env_enable_obstacle: false
env_major_point_min_distant_percent: 0.25
env_num_agent: 10
env_num_charger: 3
env_num_circle_obstacle: 1
env_num_grid: 50
env_num_major_point: 3
env_num_max_neighbor_agent: 5
env_num_max_neighbor_poi: 10
env_num_poi: 100
env_num_rect_obstacle: 1
env_observation_test: true
env_static_poi: false
env_suofang: 10
env_theta: 60
env_use_pixel_obs: true
env_use_square_obs: false
env_zmax: 15
env_zmin: 5
episode_length: 100
epsilon_decay_percent_episode: 2.0e-05
epsilon_decay_temperature: 0.0001
epsilon_exponential_decay: true
epsilon_linear_decay: false
gamma: 0.95
hidden_dim: 256
initial_epsilon: 0.9
learning_rate: 0.001
max_poi_speed: 0
min_epsilon: 0.3
num_episodes: 30000
num_max_keep_ckpt: 5
outage_probability: 0
scenario_path: scenarios.continuous_mcs3D.ContinuousMCS3D
skip_connect: true
soft_update_target_network: false
step_episode_length: 100
tau: 0.99
testing_action_mode: epsilon-greedy
testing_episodes: 20
testing_interval: 100
trainer_path: trainers.value_based_trainer.ValueBasedTrainer
training_action_mode: epsilon-greedy
training_interval: 1
training_times: 4
user_episode_length: 100
user_episodes: 10
work_dir: checkpoints/dqn
